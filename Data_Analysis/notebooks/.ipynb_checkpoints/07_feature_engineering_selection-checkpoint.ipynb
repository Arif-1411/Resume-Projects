{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 07 \u2014 Feature Engineering & Selection\n**Data Analysis Portfolio**\n\n**Engineering:** encoding, scaling, polynomial, date features, interactions, transforms\n**Selection:** variance threshold, correlation, SelectKBest, RFE, Random Forest importance"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nnp.random.seed(42)\nprint(\"All libraries loaded.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Build Dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "n = 500\ndf = pd.DataFrame({\n    'age':          np.random.randint(22,60,n),\n    'salary':       np.random.lognormal(10.8,0.4,n).round(0).clip(25000,200000),\n    'experience':   np.random.randint(0,35,n),\n    'num_projects': np.random.randint(1,20,n),\n    'department':   np.random.choice(['IT','HR','Finance','Marketing'],n),\n    'education':    np.random.choice([\"Bachelor's\",\"Master's\",\"PhD\"],n),\n    'gender':       np.random.choice(['Male','Female'],n),\n    'join_date':    pd.date_range('2015-01-01',periods=n,freq='3D'),\n    'noise_1':      np.random.randn(n),\n    'noise_2':      np.random.randn(n),\n    'constant_col': 1,\n})\nlog_odds = 0.03*df['age'] + 0.00001*df['salary'] + 0.05*df['experience'] + 0.1*df['num_projects'] - 3\ndf['promoted'] = (np.random.rand(n) < 1/(1+np.exp(-log_odds))).astype(int)\nprint(\"Shape:\", df.shape, \"| Promoted%:\", round(df['promoted'].mean()*100,1))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Feature Engineering"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fe = df.copy()\n# Numeric transforms\nfe['salary_log']         = np.log1p(fe['salary'])\nfe['experience_squared'] = fe['experience'] ** 2\n\n# Interaction features\nfe['exp_per_age']         = (fe['experience'] / fe['age']).round(4)\nfe['salary_per_project']  = (fe['salary'] / fe['num_projects']).round(2)\nfe['productivity']        = (fe['num_projects'] * fe['salary'] / 1e6).round(4)\n\n# Binning\nfe['age_group']   = pd.cut(fe['age'],  bins=[18,30,40,50,65], labels=['Junior','Mid','Senior','Expert'])\nfe['salary_tier'] = pd.qcut(fe['salary'], q=4, labels=['Q1','Q2','Q3','Q4'])\n\n# Date features\nfe['join_year']        = fe['join_date'].dt.year\nfe['join_month']       = fe['join_date'].dt.month\nfe['join_quarter']     = fe['join_date'].dt.quarter\nfe['tenure_days']      = (pd.Timestamp('2024-01-01') - fe['join_date']).dt.days\nfe['is_long_tenure']   = (fe['tenure_days'] > 1000).astype(int)\n\nprint(f\"Features: {df.shape[1]} \u2192 {fe.shape[1]}  (+{fe.shape[1]-df.shape[1]} new)\")\nprint(fe[['salary_log','exp_per_age','tenure_days','productivity']].head(3))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Encoding"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "enc = fe.copy()\n\n# Label encode \u2014 binary\nle = LabelEncoder()\nenc['gender_label']      = le.fit_transform(enc['gender'])\n\n# Ordinal encode \u2014 ordered categories\nenc['education_ordinal'] = enc['education'].map({\"Bachelor's\":0,\"Master's\":1,\"PhD\":2})\n\n# One-Hot encode \u2014 nominal\ndept_ohe = pd.get_dummies(enc['department'], prefix='dept', drop_first=True)\nenc = pd.concat([enc, dept_ohe], axis=1)\n\n# Drop originals\nenc = enc.drop(columns=['gender','department','education','join_date','age_group','salary_tier',\n                          'noise_1','noise_2','constant_col'])\n\nprint(\"Shape after encoding:\", enc.shape)\nprint(\"OHE columns:\", [c for c in enc.columns if c.startswith('dept')])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Scaling Comparison"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "scale_cols = ['age','salary','experience','num_projects','tenure_days']\nX_s = enc[scale_cols].dropna()\n\nfig, axes = plt.subplots(len(scale_cols), 4, figsize=(16, 12))\nfig.suptitle('Scaling Comparison', fontsize=13, fontweight='bold')\n\nscalers = [('Original', None), ('MinMax', MinMaxScaler()), ('Standard', StandardScaler()), ('Robust', RobustScaler())]\nfor j, col in enumerate(scale_cols):\n    for k, (name, scaler) in enumerate(scalers):\n        data = X_s[[col]]\n        vals = scaler.fit_transform(data).flatten() if scaler else data[col].values\n        axes[j,k].hist(vals, bins=20, edgecolor='white',\n                       color=['salmon','steelblue','mediumseagreen','mediumpurple'][k])\n        axes[j,k].set_ylabel(col if k==0 else '', fontsize=8)\n        if j==0: axes[j,k].set_title(name)\n\nplt.tight_layout()\nplt.savefig('/home/claude/data_analysis_portfolio/notebooks/07_scaling.png', dpi=100)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Feature Selection"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "X = enc.drop(columns=['promoted']).select_dtypes(include=[np.number]).fillna(0)\ny = enc['promoted']\nprint(\"Features:\", X.shape[1])\nprint(X.columns.tolist())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# METHOD 1 \u2014 Variance Threshold\nvt = VarianceThreshold(threshold=0.01)\nvt.fit(X)\ndropped = X.columns[~vt.get_support()].tolist()\nprint(f\"Variance Threshold \u2014 dropped {len(dropped)}: {dropped}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# METHOD 2 \u2014 Correlation Filter\ncorr  = X.corr().abs()\nupper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\nhi    = [c for c in upper.columns if any(upper[c] > 0.90)]\nprint(f\"High correlation (>0.90): {hi}\")\n\nplt.figure(figsize=(12,9))\nsns.heatmap(corr, cmap='coolwarm', linewidths=0.3, xticklabels=True, yticklabels=True)\nplt.title('Feature Correlation Matrix')\nplt.xticks(fontsize=7, rotation=45, ha='right')\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('/home/claude/data_analysis_portfolio/notebooks/07_corr.png', dpi=100)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# METHOD 3 \u2014 SelectKBest\nskb      = SelectKBest(score_func=f_classif, k=8)\nskb.fit(X, y)\nselected = X.columns[skb.get_support()].tolist()\nscores   = pd.Series(skb.scores_, index=X.columns).sort_values(ascending=False)\nprint(\"SelectKBest top 8:\", selected)\nprint(scores.head(10).round(2))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# METHOD 4 \u2014 Random Forest Feature Importance\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\nimp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10,6))\nimp.head(12).plot(kind='bar', color='steelblue', edgecolor='white')\nplt.title('Random Forest \u2014 Feature Importance', fontsize=13, fontweight='bold')\nplt.xticks(rotation=35, ha='right')\nplt.tight_layout()\nplt.savefig('/home/claude/data_analysis_portfolio/notebooks/07_importance.png', dpi=100)\nplt.show()\nprint(\"Top 5:\", imp.head(5).round(4).to_dict())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# METHOD 5 \u2014 RFE\nrfe = RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=8)\nrfe.fit(X, y)\nrfe_feats = X.columns[rfe.support_].tolist()\nprint(\"RFE selected:\", rfe_feats)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Compare All Methods"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "rf_top8 = imp.head(8).index.tolist()\nall3    = set(selected) & set(rf_top8) & set(rfe_feats)\nprint(\"=\"*55)\nprint(\"FEATURE SELECTION SUMMARY\")\nprint(\"=\"*55)\nprint(f\"Total features:        {X.shape[1]}\")\nprint(f\"SelectKBest:           {selected}\")\nprint(f\"RF Importance (top 8): {rf_top8}\")\nprint(f\"RFE:                   {rfe_feats}\")\nprint(f\"\\nConsensus features (all 3 methods): {sorted(all3)}\")\nprint(\"\\n\u2192 These are your MOST RELIABLE features for ML models.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u2705 Full Summary\n| Category | Technique | Tool |\n|----------|-----------|------|\n| **Engineering** | Log/sqrt | `np.log1p()` |\n| | Interaction | `col1/col2`, `col1*col2` |\n| | Binning | `pd.cut()`, `pd.qcut()` |\n| | Date features | `.dt.year`, tenure |\n| **Encoding** | Binary | `LabelEncoder` |\n| | Ordinal | manual `map()` |\n| | Nominal | `pd.get_dummies()` |\n| **Scaling** | Normalize | `MinMaxScaler` |\n| | Standardize | `StandardScaler` |\n| | Robust | `RobustScaler` |\n| **Selection** | Variance | `VarianceThreshold` |\n| | Correlation | manual filter |\n| | Univariate | `SelectKBest` |\n| | Wrapper | `RFE` |\n| | Embedded | `RandomForestClassifier` |"
  }
 ]
}